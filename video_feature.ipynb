{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle \n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "# Mediapipe configurations\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#!{sys.executable} -m pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_features_to_csv(df_features, file_path):\n",
    "    df_features.to_csv(file_path, index=False)\n",
    "\n",
    "# Calculate the eye aspect ratio\n",
    "def calculate_eye_aspect_ratio(eye_landmarks):\n",
    "    A = np.linalg.norm(eye_landmarks[1] - eye_landmarks[5])\n",
    "    B = np.linalg.norm(eye_landmarks[2] - eye_landmarks[4])\n",
    "    C = np.linalg.norm(eye_landmarks[0] - eye_landmarks[3])\n",
    "    return (A + B) / (2.0 * C)\n",
    "\n",
    "# Calculate the distance between two points\n",
    "def normalize_vector(v):\n",
    "    norm = np.linalg.norm(v)\n",
    "    if norm == 0:\n",
    "        return v\n",
    "    return v / norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_folder = '/Users/iduli/Desktop/Ch2_25_Scientific_Data/Final/raw_Dataset/SENSORS/VIDEO/parsed_video'\n",
    "result_folder = '/Users/iduli/Desktop/Ch2_25_Scientific_Data/Final/raw_Dataset/SENSORS/VIDEO'\n",
    "\n",
    "# Mediapipe, OpenCV and interval configurations\n",
    "EAR_THRESHOLD = 0.3\n",
    "FOURCC = 'XVID'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video(video_path):\n",
    "    frame_idx = 0\n",
    "    frame_time = 0\n",
    "    # Open video\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Cannot open video {video_path}\")\n",
    "        return None    \n",
    "    folder_path, video_file = os.path.split(video_path)\n",
    "    print(video_file)\n",
    "    base_name = os.path.splitext(video_file)[0]\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    # Initialize variables\n",
    "    feature_list = []\n",
    "    prev_lip_corner_left, prev_lip_corner_right = None, None\n",
    "    prev_left_eye_aspect_ratio, prev_right_eye_aspect_ratio = None, None\n",
    "    blink_count = 0\n",
    "\n",
    "    with mp_face_detection.FaceDetection(model_selection=1, min_detection_confidence=0.7) as face_detection, \\\n",
    "         mp_face_mesh.FaceMesh(min_detection_confidence=0.7, min_tracking_confidence=0.7, refine_landmarks= True) as face_mesh:\n",
    "        #print(frame_idx)\n",
    "        while cap.isOpened():\n",
    "            # Set frame time, will be added after 영상 시작시간  \n",
    "            #current_frame_idx = int(cap.get(cv2.CAP_PROP_POS_FRAMES))\n",
    "            #print(current_frame_idx) frame 0 1 같은시간 읽어짐. \n",
    "            frame_time = cap.get(cv2.CAP_PROP_POS_MSEC)\n",
    "            success, image = cap.read()\n",
    "            if not success:\n",
    "                # Case 1: 프레임 읽기 실패\n",
    "                frame_features = [frame_time, np.nan, np.nan, np.nan, np.nan,\n",
    "                                  np.nan, np.nan, np.nan, np.nan, np.nan, 1]  # error_type = 1\n",
    "                break\n",
    "\n",
    "            # Convert BGR to RGB for Mediapipe\n",
    "            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            image_rgb.flags.writeable = False\n",
    "\n",
    "            # Process face detection and face mesh\n",
    "            results = face_detection.process(image_rgb)\n",
    "            mesh_results = face_mesh.process(image_rgb)\n",
    "\n",
    "            if not results.detections:\n",
    "                # Case 2: 얼굴 탐지 실패\n",
    "                frame_features = [frame_time, np.nan, np.nan, np.nan, np.nan,\n",
    "                                  np.nan, np.nan, np.nan, np.nan, np.nan, 2]  # error_type = 2\n",
    "                feature_list.append(frame_features)\n",
    "                continue\n",
    "\n",
    "            # Convert RGB back to BGR for OpenCV\n",
    "            image_rgb.flags.writeable = True\n",
    "            image = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "            # Initialize default values\n",
    "            rotation_x, rotation_y, rotation_z = np.nan, np.nan, np.nan\n",
    "            total_translation = np.nan\n",
    "            gaze_direction = np.array([np.nan, np.nan])\n",
    "            left_lip_movement, right_lip_movement = np.nan, np.nan\n",
    "\n",
    "            # Process face detection\n",
    "            if results.detections:\n",
    "                largest_face = max(results.detections, key=lambda detection: detection.location_data.relative_bounding_box.width * detection.location_data.relative_bounding_box.height)\n",
    "                blink_count = 0  # Reset blink count for each frame\n",
    "\n",
    "            # Process face mesh landmarks\n",
    "            if mesh_results.multi_face_landmarks:\n",
    "                for face_landmarks in mesh_results.multi_face_landmarks:\n",
    "                    landmarks = np.array([(lm.x, lm.y, lm.z) for lm in face_landmarks.landmark])\n",
    "\n",
    "                    # Calculate head movement\n",
    "                    image_points = np.array([\n",
    "                        (landmarks[1][0] * image.shape[1], landmarks[1][1] * image.shape[0]),\n",
    "                        (landmarks[33][0] * image.shape[1], landmarks[33][1] * image.shape[0]),\n",
    "                        (landmarks[263][0] * image.shape[1], landmarks[263][1] * image.shape[0]),\n",
    "                        (landmarks[61][0] * image.shape[1], landmarks[61][1] * image.shape[0]),\n",
    "                        (landmarks[291][0] * image.shape[1], landmarks[291][1] * image.shape[0]),\n",
    "                        (landmarks[199][0] * image.shape[1], landmarks[199][1] * image.shape[0])\n",
    "                    ], dtype=\"double\")\n",
    "\n",
    "                    '''considering changing the points from static value to dynamic value based on landmarks'''\n",
    "                    model_points = np.array([\n",
    "                        (0.0, 0.0, 0.0),\n",
    "                        (-30.0, -125.0, -30.0),\n",
    "                        (30.0, -125.0, -30.0),\n",
    "                        (-60.0, -70.0, -60.0),\n",
    "                        (60.0, -70.0, -60.0),\n",
    "                        (0.0, -150.0, -100.0)\n",
    "                    ])\n",
    "\n",
    "                    size = image.shape\n",
    "                    focal_length = size[1]\n",
    "                    center = (size[1] / 2, size[0] / 2)\n",
    "                    camera_matrix = np.array([\n",
    "                        [focal_length, 0, center[0]],\n",
    "                        [0, focal_length, center[1]],\n",
    "                        [0, 0, 1]\n",
    "                    ], dtype=\"double\")\n",
    "\n",
    "                    dist_coeffs = np.zeros((4, 1))\n",
    "                    success, rotation_vector, translation_vector = cv2.solvePnP(model_points, image_points, camera_matrix, dist_coeffs)\n",
    "\n",
    "                    if success:\n",
    "                        rotation_x, rotation_y, rotation_z = rotation_vector.ravel()\n",
    "                        total_translation = np.linalg.norm(translation_vector)\n",
    "\n",
    "                        # Calculate eye aspect ratio\n",
    "                        left_eye_landmarks = landmarks[[33, 160, 158, 133, 153, 144]]\n",
    "                        right_eye_landmarks = landmarks[[362, 385, 387, 263, 373, 380]]\n",
    "                        left_ear = calculate_eye_aspect_ratio(left_eye_landmarks)\n",
    "                        right_ear = calculate_eye_aspect_ratio(right_eye_landmarks)\n",
    "\n",
    "                        if prev_left_eye_aspect_ratio is not None and prev_right_eye_aspect_ratio is not None:\n",
    "                            if (left_ear < EAR_THRESHOLD and prev_left_eye_aspect_ratio >= EAR_THRESHOLD) or \\\n",
    "                               (right_ear < EAR_THRESHOLD and prev_right_eye_aspect_ratio >= EAR_THRESHOLD):\n",
    "                                blink_count += 1\n",
    "\n",
    "                        prev_left_eye_aspect_ratio = left_ear\n",
    "                        prev_right_eye_aspect_ratio = right_ear\n",
    "\n",
    "                        # Calculate gaze direction\n",
    "                        \n",
    "                        left_iris_center = np.mean(landmarks[[474, 475, 476, 477]], axis=0)\n",
    "                        right_iris_center = np.mean(landmarks[[469, 470, 471, 472]], axis=0)\n",
    "                        nose_tip = landmarks[1]\n",
    "                        gaze_direction = normalize_vector((left_iris_center + right_iris_center) / 2.0 - nose_tip)\n",
    "\n",
    "                        # Calculate lip movement\n",
    "                        left_lip_corner = landmarks[61]\n",
    "                        right_lip_corner = landmarks[291]\n",
    "                        if prev_lip_corner_left is not None and prev_lip_corner_right is not None:\n",
    "                            left_lip_movement = np.linalg.norm(left_lip_corner - nose_tip) - np.linalg.norm(prev_lip_corner_left - nose_tip)\n",
    "                            right_lip_movement = np.linalg.norm(right_lip_corner - nose_tip) - np.linalg.norm(prev_lip_corner_right - nose_tip)\n",
    "\n",
    "                        prev_lip_corner_left = left_lip_corner\n",
    "                        prev_lip_corner_right = right_lip_corner\n",
    "            # Append features\n",
    "            frame_features = [frame_time, rotation_x, rotation_y, rotation_z, total_translation,\n",
    "                              gaze_direction[0], gaze_direction[1], left_lip_movement, right_lip_movement, blink_count]\n",
    "            feature_list.append(frame_features)\n",
    "            frame_idx += 1\n",
    "\n",
    "    cap.release()\n",
    "    csv_file_path = os.path.join(result_folder, f'{base_name}_features.csv')\n",
    "    save_features_to_csv(pd.DataFrame(feature_list, columns=[\n",
    "        \"Timestamp\", \"rotation_x\", \"rotation_y\", \"rotation_z\",\n",
    "        \"Total Movement\", \"Gaze X\", \"Gaze Y\",\n",
    "        \"Left Lip Movement\", \"Right Lip Movement\",\n",
    "        \"Blink Count\",'error_type']), csv_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test \n",
    "process_video('/Users/iduli/Desktop/Ch2_25_Scientific_Data/Final/raw_Dataset/SENSORS/VIDEO/parsed_video/3_c3.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
